# -*- coding: utf-8 -*-
"""Employee Attrition Project VIEH group.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kCdw_6lm9vl5UWqmdmNFTbLtSGeZnf_
"""



"""Data preprocessing: and including Libraries

Data Preprocessing
"""

import numpy as np
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('data.csv')

"""Data Exploration"""

dataset.head()

dataset.shape

dataset.columns

dataset.info()

#categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

dataset.select_dtypes(include='int64').columns

len(dataset.select_dtypes(include='int64').columns)

"""Statistical summary"""

dataset.describe()

"""Restructurng the dataset"""

dataset.head()

"""We're deleting these columns because these are not relatable.
EmporyeeCount,EmpoyeeNumber,Over18,StandardHours,
"""

dataset['EmployeeCount'].nunique()

dataset['EmployeeCount'].unique()

dataset['Over18'].nunique()

dataset['Over18'].unique()

dataset['StandardHours'].nunique()

dataset['StandardHours'].unique()

"""Dropping the unrelatable columns"""



dataset = dataset.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'])

dataset.info()

dataset.shape

"""Dealing with the missing values"""

dataset.isnull().values.any()

dataset.isnull().values.sum()

"""Count plot

"""

sns.countplot(dataset['Attrition'])
plt.show()

#Employees left the company
(dataset.Attrition =='Yes').sum()

#Employees with the company
(dataset.Attrition =='No').sum()

plt.figure(figsize=(20,20))
plt.subplot(311)
sns.countplot(x='Department',hue='Attrition',data=dataset)

plt.figure(figsize=(20,20))

plt.subplot(311)
sns.countplot(x='JobRole',hue='Attrition',data=dataset)

plt.figure(figsize=(20,20))

plt.subplot(311)
sns.countplot(x='JobSatisfaction',hue='Attrition',data=dataset)

"""Corellation matrix and Heatmapp

"""

corr= dataset.corr()

plt.figure(figsize=(16,9))
ax=sns.heatmap(corr,annot=True,cmap='coolwarm')



"""Dealing with categorical data"""

dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

dataset.shape

#one hot encoding
dataset = pd.get_dummies(data=dataset,drop_first=True)

dataset.shape

len(dataset.select_dtypes(include='object').columns)

dataset.head()

dataset.rename(columns={'Attrition_Yes':'Attrition'},inplace=True)

dataset.head()

"""Splitting data"""

#matrix of features
x=dataset.drop(columns='Attrition')

#target var
y=dataset['Attrition']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

x_train.shape

y_train.shape

x_test.shape

y_test.shape

"""Feature Scalling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""Building the Model

"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression
classifer_lr = LogisticRegression(random_state=0)
classifer_lr.fit(x_train, y_train)

y_pred = classifer_lr.predict(x_test)

from sklearn.metrics import accuracy_score,confusion_matrix

acc = accuracy_score(y_test, y_pred)
print(acc*100)

cm = confusion_matrix(y_test,y_pred)
print(cm)

"""Random forest"""

#Random Forest
from sklearn.ensemble import RandomForestClassifier
classifer_rf = RandomForestClassifier(random_state=0)
classifer_rf.fit(x_train, y_train)

y_pred = classifer_rf.predict(x_test)

from sklearn.metrics import accuracy_score,confusion_matrix

acc = accuracy_score(y_test, y_pred)
print(acc*100)

cm = confusion_matrix(y_test,y_pred)
print(cm)

"""SVM support vector machine"""

from sklearn.svm import SVC
classifer_svc = SVC(random_state=0)
classifer_svc.fit(x_train, y_train)

y_pred = classifer_svc.predict(x_test)

acc = accuracy_score(y_test, y_pred)
print(acc*100)

cm = confusion_matrix(y_test,y_pred)
print(cm)

"""Randomized search to find the best parameters (Logistic regression) """

from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'penalty':['l1','l2','elasticnet','none'],
    'C':[0.25,0.5,0.75,1.0,1.25,1.5,1.75,2.0],
    'solver':['newton-cg','lbfgs','liblinear','sag','saga'],
    'max_iter':[50,100,500,2000,5000]
}

random_cv = RandomizedSearchCV(estimator=classifer_lr,param_distributions=parameters,n_iter=10,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)

random_cv.fit(x_train,y_train)

random_cv.best_estimator_

random_cv.best_params_

random_cv.best_score_

"""Final Model (Logistic Regression)"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression
classifer = LogisticRegression(C=0.25, random_state=0, solver='liblinear')
classifer.fit(x_train, y_train)

y_pred = classifer.predict(x_test)

from sklearn.metrics import accuracy_score, confusion_matrix
acc=accuracy_score(y_test,y_pred)
print(acc*100)

cm=confusion_matrix(y_test,y_pred)
print(cm)

"""Predicting a single observation

"""

dataset.head()

single_obs = [[41,1102,1,2,2,94,3,2,4,5993,19479,8,11,3,1,0,8,0,1,6,4,0,5,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1]]

classifer.predict(sc.transform(single_obs))
#leaving the company if 1 else not leaving